{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Scrapy 框架介绍\n",
    "\n",
    "```\n",
    "Scrapy是用纯Python实现一个为了爬取网站数据、提取结构性数据而编写的应用框架，用途非常广泛。\n",
    "Scrapy框架：用户只需要定制开发几个模块就可以轻松的实现一个爬虫，用来抓取网页内容以及各种图片，非常之方便。\n",
    "Scrapy 使用了Twisted(其主要对手是Tornado)多线程异步网络框架来处理网络通讯，可以加快我们的下载速度，不用自己去实现异步框架，并且包含了各种中间件接口，可以灵活的完成各种需求。\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "#### Scrapy架构图 \n",
    "\n",
    "![image](scrapy数据流向.png)\n",
    "\n",
    "\n",
    "\n",
    "```python\n",
    "Scrapy主要包括了以下组件：\n",
    "\tScrapy Engine(引擎): \n",
    "        负责Spider、ItemPipeline、Downloader、Scheduler中间的通讯，信号、数据传递等。\n",
    "\n",
    "\tScheduler(调度器): \n",
    "        它负责接受`引擎`发送过来的Request请求，并按照一定的方式进行整理排列，入队，当`引擎`需要时，交还给`引擎`。\n",
    "\n",
    "\tDownloader（下载器）：\n",
    "    \t负责下载`Scrapy Engine(引擎)`发送的所有Requests请求，并将其获取到的Responses交还给`Scrapy Engine(引擎)`，由`引擎`交给`Spider`来处理，\n",
    "\n",
    "\tSpider（爬虫）：\n",
    "    \t它负责处理所有Responses,从中分析提取数据，获取Item字段需要的数据，并将需要跟进的URL提交给`引擎`，再次进入`Scheduler(调度器)`，\n",
    "\n",
    "\tItem Pipeline(管道)：\n",
    "    \t它负责处理`Spider`中获取到的Item，并进行后期处理（详细分析、过滤、存储等）的地方.\n",
    "\n",
    "\tDownloader Middlewares（下载中间件）：\n",
    "\t\t你可以当作是一个可以自定义扩展下载功能的组件。\n",
    "\n",
    "\tSpider Middlewares（Spider中间件）：\n",
    "    \t你可以理解为是一个可以自定扩展和操作`引擎`和`Spider`中间`通信`的功能组件（比如进入`Spider`的Responses和从`Spider`出去的Requests）\n",
    "        \n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 安装Scrapy\n",
    "\n",
    "```python\n",
    "Scrapy的安装介绍\n",
    "\tScrapy框架官方网址：http://doc.scrapy.org/en/latest\n",
    "\tScrapy中文维护站点：http://scrapy-chs.readthedocs.io/zh_CN/latest/index.html\n",
    "\t\n",
    "安装方式:   \n",
    "    pip install scrapy -i https://pypi.douban.com/simple\n",
    "    \n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "### 使用Scrapy\n",
    "\n",
    "#### 使用爬虫可以遵循以下步骤：\n",
    "\n",
    "1. 创建一个Scrapy项目\n",
    "2. 定义提取的Item\n",
    "3. 编写爬取网站的 spider 并提取 Item\n",
    "4. 编写 Item Pipeline 来存储提取到的Item(即数据)\n",
    "\n",
    "\n",
    "\n",
    "#### 1. 新建项目(scrapy startproject)\n",
    "\n",
    "##### 创建一个新的Scrapy项目来爬取 https://www.meijutt.tv/new100.html 中的数据，使用以下命令：\n",
    "\n",
    "```python\n",
    "scrapy startproject meiju\n",
    "```\n",
    "\n",
    "##### 创建爬虫程序\n",
    "\n",
    "```python\n",
    "cd meiju\n",
    "scrapy genspider meijuSpider meijutt.tv\n",
    "\n",
    "其中：\n",
    "\tmeijuSpider为爬虫文件名\n",
    "\tmeijutt.com为爬取网址的域名\n",
    "```\n",
    "\n",
    "创建Scrapy工程后, 会自动创建多个文件，下面来简单介绍一下各个主要文件的作用：\n",
    "\n",
    "```python\n",
    "scrapy.cfg：\n",
    "\t项目的配置信息，主要为Scrapy命令行工具提供一个基础的配置信息。（真正爬虫相关的配置信息在settings.py文件中）\n",
    "items.py：\n",
    "\t设置数据存储模板，用于结构化数据，如：Django的Model\n",
    "pipelines：\n",
    "\t数据处理行为，如：一般结构化的数据持久化\n",
    "settings.py：\n",
    "\t配置文件，如：递归的层数、并发数，延迟下载等\n",
    "spiders：\n",
    "\t爬虫目录，如：创建文件，编写爬虫规则\n",
    "    \n",
    "注意：一般创建爬虫文件时，以网站域名命名\n",
    "```\n",
    "\n",
    "#### 2. 定义Item\n",
    "\n",
    "​\tItem是保存爬取到的数据的容器；其使用方法和python字典类似，虽然我们可以在Scrapy中直接使用dict，但是 Item提供了额外保护机制来避免拼写错误导致的未定义字段错误；\n",
    "\n",
    "​\t类似ORM中的Model定义字段，我们可以通过scrapy.Item 类来定义要爬取的字段。\n",
    "\n",
    "```python\n",
    "import scrapy\n",
    "\n",
    "class MeijuItem(scrapy.Item):\n",
    "    name = scrapy.Field()\n",
    "```\n",
    "\n",
    "#### 3. 编写爬虫\n",
    "\n",
    "```python\n",
    "# -*- coding: utf-8 -*-\n",
    "import scrapy\n",
    "from lxml import etree\n",
    "from meiju.items import MeijuItem\n",
    "\n",
    "class MeijuspiderSpider(scrapy.Spider):\n",
    "    # 爬虫名\n",
    "    name = 'meijuSpider'\n",
    "    # 被允许的域名\n",
    "    allowed_domains = ['meijutt.tv']\n",
    "    # 起始爬取的url\n",
    "    start_urls = ['http://www.meijutt.tv/new100.html']\n",
    "\n",
    "    # 数据处理\n",
    "    def parse(self, response):\n",
    "        # response响应对象\n",
    "        # xpath\n",
    "        mytree = etree.HTML(response.text)\n",
    "        movie_list = mytree.xpath('//ul[@class=\"top-list  fn-clear\"]/li')\n",
    "\n",
    "        for movie in movie_list:\n",
    "            name = movie.xpath('./h5/a/text()')\n",
    "\n",
    "            # 创建item(类字典对象)\n",
    "            item = MeijuItem()\n",
    "            item['name'] = name\n",
    "            yield item\n",
    "\n",
    "```\n",
    "\n",
    "##### 启用一个Item Pipeline组件\n",
    "\n",
    "为了启用Item Pipeline组件，必须将它的类添加到 settings.py文件ITEM_PIPELINES 配置修改settings.py，并设置优先级，分配给每个类的整型值，确定了他们运行的顺序，item按数字从低到高的顺序，通过pipeline，通常将这些数字定义在0-1000范围内（0-1000随意设置，数值越低，组件的优先级越高）\n",
    "\n",
    "```python\n",
    "ITEM_PIPELINES = {\n",
    "   'meiju.pipelines.MeijuPipeline': 300,\n",
    "}\n",
    "```\n",
    "\n",
    "##### 设置UA\n",
    "\n",
    "在setting.py中设置USER_AGENT的值\n",
    "\n",
    "```python\n",
    "USER_AGENT = 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'\n",
    "\n",
    "```\n",
    "\n",
    "#### 4. 编写 Pipeline 来存储提取到的Item(即数据)\n",
    "\n",
    "```python\n",
    "class SomethingPipeline(object):\n",
    "    def __init__(self):    \n",
    "        # 可选实现，做参数初始化等\n",
    "        \n",
    "\tdef process_item(self, item, spider):\n",
    "        # item (Item 对象) – 被爬取的item\n",
    "        # spider (Spider 对象) – 爬取该item的spider\n",
    "        # 这个方法必须实现，每个item pipeline组件都需要调用该方法，\n",
    "        # 这个方法必须返回一个 Item 对象，被丢弃的item将不会被之后的pipeline组件所处理。\n",
    "        return item\n",
    "\n",
    "    def open_spider(self, spider):\n",
    "        # spider (Spider 对象) – 被开启的spider\n",
    "        # 可选实现，当spider被开启时，这个方法被调用。\n",
    "\n",
    "    def close_spider(self, spider):\n",
    "        # spider (Spider 对象) – 被关闭的spider\n",
    "        # 可选实现，当spider被关闭时，这个方法被调用\n",
    "    \n",
    "```\n",
    "##### 运行爬虫：\n",
    "\n",
    "```python\n",
    "scrapy crawl meijuSpider\n",
    "\n",
    "# nolog模式\n",
    "scrapy crawl meijuSpider --nolog  \n",
    "```\n",
    "\n",
    "scrapy保存信息的最简单的方法主要有这几种，-o 输出指定格式的文件，命令如下：\n",
    "\n",
    "```python\n",
    "scrapy crawl meijuSpider -o meiju.json\n",
    "scrapy crawl meijuSpider -o meiju.csv\n",
    "scrapy crawl dangdangSpider -o books.csv\n",
    "scrapy crawl meijuSpider -o meiju.xml\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### 练习： Scrapy爬取新浪新闻存入数据库 http://roll.news.sina.com.cn/news/gnxw/gdxw1/index_1.shtml \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
